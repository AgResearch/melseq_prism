#!/usr/bin/env Rscript
#
# Original Author Melanie Hess, 2019
# Modified to handle gtdb, and add comments and references, 2022/2023 Alan McCulloch
#
# This script assigns reads to the lowest common ancestor (LCA) of the set of taxa that it hit in the blast run (as per the MEGAN algorithm)
#
# algorithm references
# ====================
# Hess MK, et al. A restriction enzyme reduced representation sequencing approach for low-cost, high-throughput metagenome profiling. 
#  PLoS One. 2020 Apr 3;15(4):e0219882. doi: 10.1371/journal.pone.0219882. PMID: 32243481; PMCID: PMC7122713.
#  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122713/
#
# "Reads were assigned to a taxonomic node using the algorithm from MEGAN [27] implemented in R with default 
# parameters: a minimum bitscore of 50 and considering only hits within 10% of the maximum bitscore for a query read.
# 
# Huson DH, Auch AF, Qi J, Schuster SC. MEGAN analysis of metagenomic data. Genome Res. 2007;17(3):377–86. 10.1101/gr.5969107
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1800929/
#
# example usage 
# =============
#
## 1. uncompress tabular blast results
# gunzip -c 968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.results.gz  > 968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.resultsNucl
#
## 2. run script  
# Rscript --vanilla summarizeR_counts.code 968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.resultsNucl 
#    1>968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.stdout 
#    2>968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.stderr
#
## 3. remove uncompressed version 
# /usr/bin/rm -f 968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.resultsNucl
#
# This example would write taxonomy summary file  
#  968854_TGAAGCG_psti.R1_trimmed.fastq.non-redundant.fasta.blastn.GTDB1.summary
#
# Example blast input (tabular format from blastn with argument -outfmt '6 std qlen' )
#Sequence58_count=1      GTDB1:GCA_016297855.1_JAEDFW010000006.1 87.234  47      5       1       1       47      24786   24741   1.09e-04        55.4    58
#Sequence58_count=1      GTDB1:GCA_017395685.1_JAFQOD010000004.1 91.892  37      3       0       4       40      95169   95205   3.81e-04        54.5    58
#Sequence58_count=1      GTDB1:GCA_017523605.1_JAFWVZ010000201.1 84.444  45      7       0       5       49      4784    4740    0.005   50.9    58
#Sequence58_count=1      GTDB1:GCA_017619905.1_JAFZRK010000080.1 82.979  47      8       0       3       49      619     665     0.005   50.0    58
#Sequence58_count=1      GTDB1:GCF_018854495.1_NZ_JABBDR010000079.1      91.176  34      3       0       3       36      13306   13339   0.016   49.1    58
#Sequence59_count=2      GTDB1:GCA_902764875.1_CACWXP010000004.1 98.276  58      1       0       1       58      173089  173146  2.93e-18        101     58
#Sequence61_count=2      GTDB1:GCA_003266105.1_MVAA01000044.1    100.000 45      0       0       1       45      2479    2435    3.93e-13        82.4    45
#Sequence62_count=3      GTDB1:GCA_002368955.1_DFDY01000138.1    83.562  73      12      0       1       73      17419   17491   1.56e-11        78.8    73
#Sequence62_count=3      GTDB1:GCF_003719825.1_NZ_RIBS01000007.1 100.000 32      0       0       37      68      34000   33969   1.46e-05        59.0    73
#Sequence62_count=3      GTDB1:GCF_000622145.1_NZ_JHWH01000009.1 83.673  49      7       1       21      69      34853   34900   0.008   50.0    73
#Sequence62_count=3      GTDB1:GCA_002360945.1_DERO01000013.1    82.979  47      8       0       16      62      14898   14852   0.008   50.0    73
#
# Example summary output (output records cloned to match count header - this was originally for the benefit of some downstream processing)
#Sequence58_count=1      Bacteria        Firmicutes_A    Clostridia      NA      NA      NA      NA
#Sequence59_count=2      Bacteria        Firmicutes_A    Clostridia      Christensenellales      CAG-74  GCA-900199385   GCA-900199385 sp902764875
#Sequence59_count=2      Bacteria        Firmicutes_A    Clostridia      Christensenellales      CAG-74  GCA-900199385   GCA-900199385 sp902764875
#Sequence61_count=2      Archaea Methanobacteriota       Methanobacteria Methanobacteriales      Methanobacteriaceae     Methanosphaera  Methanosphaera sp003266105
#Sequence61_count=2      Archaea Methanobacteriota       Methanobacteria Methanobacteriales      Methanobacteriaceae     Methanosphaera  Methanosphaera sp003266105
#Sequence62_count=3      Bacteria        Bacteroidota    Bacteroidia     Bacteroidales   P3      UBA1711 UBA1711 sp002368955
#Sequence62_count=3      Bacteria        Bacteroidota    Bacteroidia     Bacteroidales   P3      UBA1711 UBA1711 sp002368955
#Sequence62_count=3      Bacteria        Bacteroidota    Bacteroidia     Bacteroidales   P3      UBA1711 UBA1711 sp002368955
#
#
# 
# To Do 
# ======
# 1. Currently this is very slow and may take over 24 hours to process all the hits for a sample. The problem will be the merge between the big taxonomy data frame (317,543 rows), and the 
#    hits for each query - i.e. the line "d2 = merge(dat2[,c(1,14)],t)" below. The previous Hungate based taxonomy file had only 419 rows ! 
#
#    Suggestion for a minimal refactoring of this script: "pre-merge" the blast hits with the taxonomy, so we can delete that line. So we would : 
# 
#    * load the gtdb taxonomy file into a database, and index the accession column on which the merge is done. That is a one-off setup
#
#    Then in the pipeline : 
# 
#    * preprocess the blast hit file with new script which will append the taxonomy of each accession, to the blast hit file - this script will use a SQL query to look up the taxonomy for each record and will be fast due to the indexing.
#    * the present script then no longer needs the line that does the merge, as the blast hits read in will already be merged
# 


args = commandArgs(trailingOnly=TRUE)
if (length(args)==0) {
  stop("Please provide the base name of the .resultsNucl file you wish to summarize.", call.=FALSE)
} 

file = args[1]
#inputFile = paste(file,".resultsNucl",sep="")
#outputFile = paste(file,".summary",sep="")

inputFile = file
base=gsub(pattern="\\.resultsNucl$","",inputFile)
outputFile = paste(base,".summary",sep="")
bitScoreCutoff = 50			# remove matches less than this 
bitScoreThreshold = 0.1		        # keep matches with bitscores within x proportion of max bitscore


getID = function(st){
   #print(paste("DEBUG st=", st))
   st = strsplit(as.character(st[1]),":")[[1]][2]
   #print(paste("DEBUG st=", st))
   a = strsplit(as.character(st[1]),"_")[[1]]
   #print(paste("DEBUG a=", a))
   #print(paste("DEBUG a[-length(a)]=", a[-length(a)]))
   #paste(a[-length(a)],collapse="_")
   paste(a[1],a[2],sep="_")
}
lengthUnique = function(dat){
   length(unique(dat))
}

collateResults = function(dat){
   #
   # this method applies the LCA algorithm to all the hits of a given query sequence (as contained in "dat")
   # 
   dat = as.data.frame(dat)
   if(ncol(dat) == 1) dat = as.data.frame(t(dat))                   # not quite sure why this needed
   colnames(dat) = c("qseqid","sseqid","pident","length","mismatch","gapopen","qstart","qend","sstart","send","evalue","bitscore","qlength")
   maxBitscore = max(as.numeric(as.character(dat$bitscore)))        # get max bitscore of all hits 
   #print(paste("DEBUG maxBitscore=", maxBitscore))
   bitThresh = round(maxBitscore*(1-bitScoreThreshold))             
   if(maxBitscore >= bitScoreCutoff){                               # so there will be some queries for which there will be no output (no hits above maxBitscore)
      dat2 = dat[as.numeric(as.character(dat$bitscore)) >= max(bitScoreCutoff,bitThresh),]   # only retain decent hits 
      if(nrow(dat2)>0){                                             # but nrow shouldn't be = 0 ?  -however no harm
         dat2$ID = apply(dat2[,2:3],1,getID)                        # get the ID of the accession 
         #print(paste("DEBUG dat2$ID=", dat2$ID))
         d2 = merge(dat2[,c(1,14)],t)                               # do an inner join to the big taxonomy array - this must be very slow now !  The gtdb taxonomy array has 317,543 rows 
         same = (apply(d2[,3:9],2,lengthUnique)==1 & !is.na(d2[1,3:9]))   # The guts of the LCA algroithm - counting number of distinct species, genus etc. Only if it is 1 do we assign at that level, else it is NA
         outDat = d2[1,-1]
         if(sum(same)<7) outDat[(min(which(same == F))+1):length(outDat)] = "NA"
         count = as.numeric(strsplit(as.character(outDat$qseqid[1]),"count=")[[1]][2])
         o=c()
         for(i in 1:count){
            o = rbind(o,outDat)
         }
         write.table(o,file=outputFile,append=T,row.names=F,col.names=F,quote=F,sep="\t")
      }
   }
}

oldQuery = "temp"
dat = c()
#
# read in the accession-to-taxonomy database. Example contents : 
#ID,Species,Genus,T_Kingdom,T_Phylum,T_Class,T_Order,T_Family,T_Genus,T_Species
#GCF_000566285.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
#GCF_003460375.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
#GCF_008388435.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
#GCF_003000855.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
#GCA_904810065.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
#GCF_002513785.1,Escherichia coli,Escherichia,Bacteria,Proteobacteria,Gammaproteobacteria,Enterobacterales,Enterobacteriaceae,Escherichia,Escherichia coli
# ...etc
# 
tax = read.csv("/dataset/gseq_processing/scratch/melseq/gtdb/GTDB1_taxonomy.csv",header=T)
t = tax[,-(2:3)]               # snip out the redundant columns 2 and 3
#write("Query\tKingdom\tPhylum\tClass\tOrder\tFamily\tGenus\tSpecies",file=outputFile)

# read the blast hits grouping by query, pass all the hits for each query to collate Results function 
inputCon = file(inputFile, open = "r")
while (length(ln <- readLines(inputCon, n = 1, warn = FALSE)) > 0) {
   q = strsplit(ln,"\t")[[1]]
   if(oldQuery == "temp") oldQuery = q[1]
   if(q[1] == oldQuery){
      dat = rbind(dat,q)
   }else{
      #print("DEBUG : collating these results")
      #print(dat)
      collateResults(dat)
      dat = q
      oldQuery = q[1]
   }
} 
collateResults(dat)
close(inputCon)
